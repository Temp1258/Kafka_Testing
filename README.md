# Kafka_Testing
Kafka Practices including:  1. Kafka + Python + PostgreSQL streaming pipeline: reads CSV, streams to Kafka, stores in PostgreSQL (aggregated &amp; raw). 2. Real-time CDC pipeline using Kafka and Python: tracks PostgreSQL changes and syncs them to a target table via Kafka.

# Kafka + PostgreSQL Streaming Pipeline Project

This project demonstrates a complete data streaming pipeline using **Kafka**, **Python**, and **PostgreSQL**, from ingestion to processing and storage. It was developed as part of a Data Engineering hands-on assignment.

---

## ğŸ“¦ Project Overview
- **Input**: `Employee_Salaries.csv` - Raw salary data file
- **Producer**: Filters and sends data to Kafka topic `employee_salaries`
- **Consumer**: Reads from Kafka and writes to PostgreSQL:
  - `department_employee_salary`: Aggregated total salary by department
  - `employee_B`: Raw records with department and salary

---

## ğŸ”§ Technologies Used
- **Apache Kafka** (`confluentinc/cp-kafka` Docker image)
- **Zookeeper** (`confluentinc/cp-zookeeper` Docker image)
- **PostgreSQL** (`postgres:14-alpine` Docker image)
- **Python 3.13** with `confluent-kafka`, `psycopg2`, `pandas`
- **Docker Desktop** for container orchestration
- **DBeaver** for PostgreSQL GUI

---

## ğŸ—‚ï¸ Project Structure
```text
â”œâ”€â”€ docker-compose.yml           # Kafka + Zookeeper + PostgreSQL setup
â”œâ”€â”€ producer.py                  # Reads CSV, filters, sends to Kafka
â”œâ”€â”€ consumer.py                  # Consumes Kafka messages and stores in Postgres
â”œâ”€â”€ employee.py                  # Defines Employee class with serialization
â”œâ”€â”€ Employee_Salaries.csv        # Input CSV file
â”œâ”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸš€ How It Works
### ğŸ” Producer Workflow (`producer.py`)
1. Reads and filters CSV rows:
   - Keeps only `ECC`, `CIT`, `EMS` departments
   - Filters out employees hired before 2010
   - Rounds down salary to integer
2. Sends messages to Kafka topic `employee_salaries`

### ğŸ§¾ Consumer Workflow (`consumer.py`)
1. Reads messages from topic `employee_salaries`
2. Parses employee JSON records
3. Updates PostgreSQL:
   - Inserts or updates `department_employee_salary` (sum of salaries per dept)
   - Appends raw data to `employee_B`

---

## ğŸ³ Running the Project (Quick Guide)

### 1. Start Docker Containers
```bash
docker-compose up -d
```

### 2. Run Producer
```bash
python producer.py
```

### 3. Run Consumer
```bash
python consumer.py
```

### 4. View Results in PostgreSQL (via DBeaver)
- Host: `localhost`
- Port: `5432`
- Database: `postgres`
- User: `postgres`
- Password: `postgres`
- Tables:
  - `department_employee_salary`
  - `employee_B`

---

## ğŸ§ª Sample Output
Example query:
```sql
SELECT * FROM department_employee_salary;
```
| department | total_salary |
|------------|--------------|
| ECC        | 4281332      |
| CIT        | 13869807     |
| EMS        | 6127454      |

---

## âœï¸ Author
- Diwen Liu (Project by BeaconFire Training)

---

## ğŸ“„ License
MIT License - feel free to reuse and adapt!


